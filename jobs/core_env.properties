##################################
######### Product Version#########
##################################
prod.maj.version=2
prod.min.version=0
prod.build.no=276

##################################
######### DB properties #########
##################################
jdbc.driver=org.postgresql.Driver
jdbc.url=jdbc:postgresql://localhost:5432/podium_md
jdbc.user=postgres
jdbc.password=#0{hPZL2FC+B3O0gSuqrA2vWg==}

##################################
#### Hibernate Configuration #####
##################################
hibernate.dialect=org.hibernate.dialect.PostgreSQLDialect
hibernate.show_sql=false
#hibernate.hbm2ddl.auto=update
entitymanager.packages.to.scan=com.nvs.core
##################################
######### Global params #########
##################################
# this is the base/root of podium directory structure for any of the data depots -
# all data is stored/manipulated relative to this base -
# CAUTION: changing this value after some data has been posted to podium will require
# an upgrade script i.e. existing data in podium will have to moved to the new base as
# a result of this property change
podium.base=podium

#Hive driver class
jdbc.hive.driver=org.apache.hive.jdbc.HiveDriver

#location of podium install - not currently used
podium.installDir=

# Command line classes i.e. class names implementing the CLI related interface and will be registered with PODIUM command line
podium.cli.cmds=com.nvs.core.init.cli.MetaLoaderCommandMgr,com.nvs.core.init.cli.DataMoverCommandMgr

#global setting for performing profiling of data - valid values:true/false
enable.profiling=true

#global setting for performing validation of data - valid values:true/false
enable.validation=true

#global setting for archiving of data - valid values:true/false
enable.archiving=true

#key part used for encryption of sensitive metadata
meta.encryption.keypart1=pd2014v1

#If true, then product validates PODIUM schema version at startup and in case of a mismatch will fail to start
validate.schema.version=true

#If true, then product at start up will automatically try to upgrade the PODIUM schema to latest available version
auto.upgrade.schema=true

##################################
##### Loading dock info ##########
##################################
# base uri without any specific path into the depot
# CAUTION: modify with caution - changing this value after some data has been posted to podium will require
# an upgrade script i.e. existing data in loading dock will have to moved to the new uri
loadingdock.uri=hdfs://10.57.171.131:8020/
# username, if applicable, as associated with the connection uri above
loadingdock.username=
# password, if applicable, as associated with the user above
loadingdock.password=
# this is the base/root of loading dock. Combined with 'podium.base' property
# this value provides a full path into the loading dock area/depot.
# CAUTION: changing this value after some data has been posted to podium will require
# an upgrade script i.e. existing data in loading dock will have to moved to the new base as
# a result of this property change
loadingdock.base=loadingdock
#default is no compression, supported methods are NONE, GZIP, DARE
loadingdock.compression=
#default is no encryption, supported methods are NONE, DARE
loadingdock.encryption=
#use threaded buffer output stream
loadingdock.use.threadedBufferedOutputStream=false

##################################
####### archive info #############
##################################
# base uri without any specific path into the depot
# CAUTION: modify with caution - changing this value after some data has been posted to podium will require
# an upgrade script i.e. existing data in archive will have to moved to the new uri
archive.uri=hdfs://10.57.171.131:8020/
# username, if applicable, as associated with the connection uri above
archive.username=
# password, if applicable, as associated with the user above
archive.password=
# this is the base/root of archive data depot. When combined with 'podium.base' property,
# this value provides a full path into the archive area/depot.
# CAUTION: modify with caution - changing this value after some data has been posted to podium will require
# an upgrade script i.e. existing data in archive will have to moved to the new base as
# a result of this property change
archive.base=archive
archive.compression=GZIP
archive.encryption=
#use threaded buffer output stream
archive.use.threadedBufferedOutputStream=false

##################################
########## receiving info ########
##################################
# base uri without any specific path into the depot
# CAUTION: modify with caution - changing this value after some data has been posted to podium will require
# an upgrade script i.e. existing data in receiving will have to moved to the new uri
receiving.uri=hdfs://10.57.171.131:8020/
# username, if applicable, as associated with the connection uri above
receiving.username=
# password, if applicable, as associated with the user above
receiving.password=
receiving.base=receiving
# this is the base/root of receiving data depot. When combined with 'podium.base' property,
# this value provides a full path into the receiving area/depot.
# CAUTION: modify with caution - changing this value after some data has been posted to podium will require
# an upgrade script i.e. existing data in receiving will have to moved to the new base as
# a result of this property change
receiving.compression=
receiving.encryption=

#export props
podium.default.export.target.location = /tmp/podium/export
podium.hive.udf.jar.location = /usr/local/podium/hive_jars/podiumhudf.jar
#receiving props
receiving.use.threadedBufferedOutputStream=false
receiving.use.threadedBufferedInputStream=false
#new shipping dock area props
shippingdock.uri=hdfs://10.57.171.131:8020/
shippingdock.username=
shippingdock.password=
shippingdock.base=shipping


##################################
########## distribution info ########
##################################
distribution.uri=jdbc:hive2://10.57.171.131:10000/default
# username, if applicable, as associated with the connection uri above
distribution.username=hdfs
# password, if applicable, as associated with the user above
distribution.password=

##################################
########## Interactive info ########
##################################
interactive.query.data.uri=jdbc:hive2://10.57.171.131:10000/default
# username, if applicable, as associated with the connection uri above
interactive.query.data.username=hdfs
# password, if applicable, as associated with the user above
interactive.query.data.password=
# Driver for Interactive Query
interactive.query.tool.driver=org.apache.hive.jdbc.HiveDriver

#############
# PIG ####
############
pig.udf.discovery.basepackages=org.apache.pig.builtin,com.podiumdata.pigudf

#############
# Kerberos ##
#############
isKerberosAuthEnabled=false
java.security.krb5.realm=TOYSTORY.REALM
java.security.krb5.kdc=woody.podiumdata.net
debug=false
useFirstPass=true
doNotPrompt=true

##################################
###### HADOOP conf directory #####
##################################
# CSV list of hadoop configuration files - absolute if paths start with /; relative otherwise
#hadoop.conf.files=conf/donald.conf/core-site.xml,conf/donald.conf/hdfs-site.xml,conf/donald.conf/mapred-site.xml
hadoop.conf.files=/usr/local/podium/hadoop-conf-files/core-site.xml,/usr/local/podium/hadoop-conf-files/hdfs-site.xml,/usr/local/podium/hadoop-conf-files/mapred-site.xml,/usr/local/podium/hadoop-conf-files/yarn-site.xml
# Path to jar files to be distributed - if empty, then path is computed dynamically by examining the class loader
#hadoop.lib.jars=/Users/atif/nvs/springWorkSpace/core/target/core-0.0.1-jar-with-dependencies.jar
hadoop.lib.jars=

################################################################

